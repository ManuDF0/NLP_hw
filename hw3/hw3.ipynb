{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import tqdm\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('yelp_reviews_train.csv', 'r') as csv_file:\n",
    "    file = csv.reader(csv_file)\n",
    "    for row in file:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting words:: 100%|██████████| 8137/8137 [00:00<00:00, 35171.06it/s]\n"
     ]
    }
   ],
   "source": [
    "no_pre = []\n",
    "for row in tqdm.tqdm(data, desc = 'Getting words:'):\n",
    "    text = row[0]\n",
    "    label = row[1]\n",
    "    words = text.lower().split(' ')\n",
    "    no_pre.append((words, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = list(range(len(no_pre)))\n",
    "for j in range(0,5):\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    for i in IDX:\n",
    "        if i%5==j:\n",
    "            test_set.append(no_pre[i])\n",
    "        else:\n",
    "            train_set.append(no_pre[i])      \n",
    "    print(f'Train set N°{j+1}')\n",
    "    priors_count = {} # Primero cuento la cantidad de buenas reviews y de malas reviews\n",
    "    vocabulary = []\n",
    "    likelihood_count = {}\n",
    "\n",
    "    for words, label in tqdm.tqdm(train_set, desc = 'Training'):\n",
    "        if label not in priors_count:\n",
    "            priors_count[label] = 0 # High y low van a ser los keys del diccionario\n",
    "        priors_count[label] +=1 # Cuento cuantos high y cuantos low\n",
    "        if label not in likelihood_count:\n",
    "            likelihood_count[label] = {} # Armo un diccionario que tiene como keys high o low\n",
    "        for word in words:\n",
    "            vocabulary.append(word) # Agrego cada palabra que veo a mi vocabulario\n",
    "            if word not in likelihood_count[label]: # Si la palabra no está en el diccionario de esa clase, la agrego\n",
    "                likelihood_count[label][word] = 0\n",
    "            likelihood_count[label][word] += 1 # Cuento cuantas veces aparece esa palabra en esa clase\n",
    "    \n",
    "    priors_probs = {} # Calculo las probabilidades de cada label (high o low)\n",
    "    for cls in priors_count: # itero sobre los keys del diccionario (high o low)\n",
    "        priors_probs[cls] = priors_count[cls]/sum(priors_count.values()) # la probabilidad de una clase va a ser #clase/(#high + #low)\n",
    "    \n",
    "    # Tenemos una lista con todas las palabras que observamos\n",
    "    # Pero en una lista pueden estar repetidas\n",
    "    vocabulary = list(set(vocabulary)) # Tranformamos la lista en un set para eliminar las repetidas\n",
    "    likelihood = {}\n",
    "    for cls in likelihood_count: # Itero sobre los high y los low\n",
    "        likelihood[cls] = {} # Para cada clase armo un diccionario\n",
    "        for word in vocabulary:\n",
    "            if word not in likelihood_count[cls]: # Si esa palabra que está en nuestro vocabulario no aparece nunca en esa clase, le ponemos un valor muy chico\n",
    "                likelihood[cls][word] = 0 + 1/(sum(likelihood_count[cls].values()) + len(vocabulary)) # Usamos el Laplace Smoothing para que no nos haga 0 nuestras probabilidades\n",
    "            else:\n",
    "                likelihood[cls][word] = (likelihood_count[cls][word]+1)/(sum(likelihood_count[cls].values()) + len(vocabulary)) # Si esa palabra si está en esa clase, calculamos la probabilidad condicional\n",
    "    # Quiero testear el accuracy\n",
    "    accuracy = 0 # Lo fijo en 0\n",
    "    total_sentences = 0\n",
    "    tp = 0 # true positives\n",
    "    positives = 0 # positive predictions\n",
    "    priors_count_test = {}\n",
    "    \n",
    "    for words, label in tqdm.tqdm(test_set, desc = 'Testing'): # Agarro mi test_set\n",
    "        if label not in priors_count_test:\n",
    "            priors_count_test[label] = 0 # High y low van a ser los keys del diccionario\n",
    "        priors_count_test[label] +=1 # Cuento cuantos high y cuantos low\n",
    "        \n",
    "        calc_prob = {}\n",
    "        for cls in priors_count: # Para cada high y low\n",
    "            if cls not in calc_prob:\n",
    "                calc_prob[cls] = math.log(priors_probs[cls]) # Calculo la probabilidad de cada clase en logs para que no se me haga 0 tan rápido\n",
    "            for word in words: # Itero sobre las palabras\n",
    "                if word in vocabulary: # Si está en el vocabulario es porque ya la vi anteriormente\n",
    "                    calc_prob[cls] = calc_prob[cls] + math.log(likelihood[cls][word]) # Si la vi anteriormente, entonces sumo la probabilidad de esa clase con la probabilidad de esa palabra\n",
    "        # Esto último es la probabilidad de la clase, dadas las palabras\n",
    "        \n",
    "        if calc_prob['high'] > calc_prob['low']: # Veo si la probabilidad de high o de low es mas alta\n",
    "            prediction = 'high' # Obtengo mis predicciones\n",
    "            positives += 1\n",
    "        else:\n",
    "            prediction = 'low'\n",
    "        if prediction == label: # Las testeo contra lo observado\n",
    "            accuracy += 1 # Cuento a cuantas le pego\n",
    "        if prediction == 'high' and label == 'high':\n",
    "            tp += 1\n",
    "        total_sentences += 1 # Cuento el total         \n",
    "    acc_total = accuracy/total_sentences\n",
    "    precision = tp/positives\n",
    "    recall = tp/priors_count_test['high']\n",
    "    f1 = 2*(precision*recall)/(precision + recall)\n",
    "    print(f'La accuracy total es de {acc_total}')\n",
    "    print(f'La precision es de {precision}')\n",
    "    print(f'El recall es de {recall}')\n",
    "    print(f'El F1 score es de {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, the model is pretty basic, we tokenize by splitting each sentence by its spaces and we don't remove punctuation. Still it works really well? If we were to predict 'high' every time, we would be right 82% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lemmatize = True, remove_stopwords = False):\n",
    "    doc = nlp(text.lower())\n",
    "    if lemmatize:\n",
    "        if remove_stopwords:\n",
    "            return [token.lemma_ for token in doc if not token.is_stop]\n",
    "        return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Data: 100%|██████████| 8137/8137 [03:57<00:00, 34.20it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = []\n",
    "for row in tqdm.tqdm(data, desc=\"Preprocessing Data\"):\n",
    "    preprocessed_data.append((preprocess_text(row[0], lemmatize = True, remove_stopwords = False), row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = list(range(len(preprocessed_data)))\n",
    "for j in range(0,5):\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    for i in IDX:\n",
    "        if i%5==j:\n",
    "            test_set.append(preprocessed_data[i])\n",
    "        else:\n",
    "            train_set.append(preprocessed_data[i])      \n",
    "    print(f'Train set N°{j+1}')\n",
    "    priors_count = {} \n",
    "    vocabulary = []\n",
    "    likelihood_count = {}\n",
    "\n",
    "    for words, label in tqdm.tqdm(train_set, desc = 'Training'):\n",
    "        if label not in priors_count:\n",
    "            priors_count[label] = 0 \n",
    "        priors_count[label] +=1\n",
    "        if label not in likelihood_count:\n",
    "            likelihood_count[label] = {}\n",
    "            \n",
    "        for word in words:\n",
    "            vocabulary.append(word) \n",
    "            if word not in likelihood_count[label]: \n",
    "                likelihood_count[label][word] = 0\n",
    "            likelihood_count[label][word] += 1\n",
    "    \n",
    "    priors_probs = {}\n",
    "    for cls in priors_count:\n",
    "        priors_probs[cls] = priors_count[cls]/sum(priors_count.values()) \n",
    "        \n",
    "    vocabulary = list(set(vocabulary))\n",
    "    likelihood = {}\n",
    "    for cls in likelihood_count: \n",
    "        likelihood[cls] = {} \n",
    "        for word in vocabulary:\n",
    "            if word not in likelihood_count[cls]: \n",
    "                likelihood[cls][word] = 0 + 1/(sum(likelihood_count[cls].values()) + len(vocabulary)) \n",
    "            else:\n",
    "                likelihood[cls][word] = (likelihood_count[cls][word] + 1)/(sum(likelihood_count[cls].values()) + len(vocabulary)) \n",
    "    accuracy = 0 \n",
    "    total_sentences = 0\n",
    "    tp = 0 \n",
    "    positives = 0 \n",
    "    priors_count_test = {}\n",
    "    for words, label in tqdm.tqdm(test_set, desc = 'Testing'): \n",
    "\n",
    "        if label not in priors_count_test:\n",
    "            priors_count_test[label] = 0\n",
    "        priors_count_test[label] +=1 \n",
    "        \n",
    "        calc_prob = {}\n",
    "        for cls in priors_count: \n",
    "            if cls not in calc_prob:\n",
    "                calc_prob[cls] = math.log(priors_probs[cls])\n",
    "            for word in words: \n",
    "                if word in vocabulary: \n",
    "                    calc_prob[cls] = calc_prob[cls] + math.log(likelihood[cls][word]) \n",
    "                         \n",
    "        if calc_prob['high'] > calc_prob['low']:\n",
    "            prediction = 'high' \n",
    "            positives += 1\n",
    "        else:\n",
    "            prediction = 'low'\n",
    "        if prediction == label: \n",
    "            accuracy += 1 \n",
    "        if prediction == 'high' and label == 'high':\n",
    "            tp += 1\n",
    "        total_sentences += 1          \n",
    "    acc_total = accuracy/total_sentences\n",
    "    precision = tp/positives\n",
    "    recall = tp/priors_count_test['high']\n",
    "    f1 = 2*(precision*recall)/(precision + recall)\n",
    "    print(f'La accuracy total es de {acc_total}')\n",
    "    print(f'La precision es de {precision}')\n",
    "    print(f'El recall es de {recall}')\n",
    "    print(f'El F1 score es de {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we improved tokenization and we are using lemmatization, but we aren't removing the stopwords. This model is slightly better than the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Data: 100%|██████████| 8137/8137 [04:02<00:00, 33.57it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = []\n",
    "for row in tqdm.tqdm(data, desc=\"Preprocessing Data\"):\n",
    "    preprocessed_data.append((preprocess_text(row[0], lemmatize = True, remove_stopwords = True), row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = list(range(len(preprocessed_data)))\n",
    "for j in range(0,5):\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    for i in IDX:\n",
    "        if i%5==j:\n",
    "            test_set.append(preprocessed_data[i])\n",
    "        else:\n",
    "            train_set.append(preprocessed_data[i])      \n",
    "    print(f'Train set N°{j+1}')\n",
    "    \n",
    "    priors_count = {}\n",
    "    likelihood_count = {}\n",
    "    vocabulary = []\n",
    "    \n",
    "    for words, label in tqdm.tqdm(train_set, desc = 'Training'):\n",
    "        if label not in priors_count:\n",
    "            priors_count[label] = 0 \n",
    "        priors_count[label] += 1 \n",
    "        if label not in likelihood_count:\n",
    "            likelihood_count[label] = {} \n",
    "        for punc in string.punctuation:\n",
    "            text = text.replace(punc,'')\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for word in words:\n",
    "            vocabulary.append(word) \n",
    "            if word not in likelihood_count[label]: \n",
    "                likelihood_count[label][word] = 0\n",
    "            likelihood_count[label][word] += 1 \n",
    "    \n",
    "    vocabulary = list(set(vocabulary)) \n",
    "    priors_probs = {} \n",
    "    for cls in priors_count: \n",
    "        priors_probs[cls] = priors_count[cls]/sum(priors_count.values()) \n",
    "    likelihood = {}\n",
    "    for cls in likelihood_count: \n",
    "        likelihood[cls] = {}\n",
    "        for word in vocabulary:\n",
    "            if word not in likelihood_count[cls]: \n",
    "                likelihood[cls][word] = 0 + 1/(sum(likelihood_count[cls].values()) + len(vocabulary))\n",
    "            else:\n",
    "                likelihood[cls][word] = (likelihood_count[cls][word]+1)/(sum(likelihood_count[cls].values()) + len(vocabulary))   \n",
    "    \n",
    "    accuracy = 0 \n",
    "    total_sentences = 0\n",
    "    tp = 0 \n",
    "    positives = 0 \n",
    "    priors_count_test = {}\n",
    "    for words, label in tqdm.tqdm(test_set, desc = 'Testing'):\n",
    "\n",
    "        if label not in priors_count_test:\n",
    "            priors_count_test[label] = 0\n",
    "        priors_count_test[label] +=1 \n",
    "        \n",
    "        calc_prob = {}\n",
    "        for cls in priors_count:\n",
    "            if cls not in calc_prob:\n",
    "                calc_prob[cls] = math.log(priors_probs[cls]) \n",
    "            for word in words: \n",
    "                if word in vocabulary: \n",
    "                    calc_prob[cls] = calc_prob[cls] + math.log(likelihood[cls][word])\n",
    "        \n",
    "        if calc_prob['high'] > calc_prob['low']: \n",
    "            prediction = 'high'\n",
    "            positives += 1\n",
    "        else:\n",
    "            prediction = 'low'\n",
    "        if prediction == label: \n",
    "            accuracy += 1\n",
    "        if prediction == 'high' and label == 'high':\n",
    "            tp += 1\n",
    "        total_sentences += 1          \n",
    "    acc_total = accuracy/total_sentences\n",
    "    precision = tp/positives\n",
    "    recall = tp/priors_count_test['high']\n",
    "    f1 = 2*(precision*recall)/(precision + recall)\n",
    "    print(f'La accuracy total es de {acc_total}')\n",
    "    print(f'La precision es de {precision}')\n",
    "    print(f'El recall es de {recall}')\n",
    "    print(f'El F1 score es de {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
