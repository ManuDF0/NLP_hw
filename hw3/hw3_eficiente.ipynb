{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import tqdm\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('yelp_reviews_train.csv', 'r') as csv_file:\n",
    "    file = csv.reader(csv_file)\n",
    "    for row in file:\n",
    "        data.append(row)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['variation', 'iteration', 'accuracy', 'precision', 'recall', 'F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting words: 100%|██████████| 8137/8137 [00:00<00:00, 48931.10it/s]\n"
     ]
    }
   ],
   "source": [
    "no_pre = []\n",
    "for row in tqdm.tqdm(data, desc = 'Getting words'):\n",
    "    text = row[0]\n",
    "    label = row[1]\n",
    "    words = text.lower().split(' ')\n",
    "    no_pre.append((words, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data, j, variation = None):\n",
    "    IDX = list(range(len(data)))\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    \n",
    "    for i in IDX:\n",
    "        if i % 5 == j:\n",
    "            test_set.append(data[i])\n",
    "        else:\n",
    "            train_set.append(data[i])\n",
    "    \n",
    "    print(f'Train set N°{j+1}')\n",
    "    \n",
    "    priors_count = defaultdict(int)\n",
    "    likelihood_count = defaultdict(Counter)\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for words, label in tqdm.tqdm(train_set, desc = 'Training'):\n",
    "        priors_count[label] += 1\n",
    "        likelihood_count[label].update(words)\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    vocabulary = list(vocabulary)\n",
    "    priors_probs = {cls: priors_count[cls] / sum(priors_count.values()) for cls in priors_count}\n",
    "\n",
    "    likelihood = defaultdict(lambda: defaultdict(float))\n",
    "    for cls in likelihood_count:\n",
    "        total_words_in_class = sum(likelihood_count[cls].values())\n",
    "        denom = total_words_in_class + len(vocabulary)\n",
    "        for word in vocabulary:\n",
    "            likelihood[cls][word] = (likelihood_count[cls][word] + 1) / denom\n",
    "\n",
    "    accuracy = 0\n",
    "    total_sentences = 0\n",
    "    tp = 0\n",
    "    positives = 0\n",
    "    priors_count_test = defaultdict(int)\n",
    "\n",
    "    for words, label in tqdm.tqdm(test_set, desc = 'Testing'):\n",
    "        priors_count_test[label] += 1\n",
    "        calc_prob = {cls: math.log(priors_probs[cls]) for cls in priors_count}\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                for cls in priors_count:\n",
    "                    calc_prob[cls] += math.log(likelihood[cls][word])\n",
    "\n",
    "        prediction = 'high' if calc_prob['high'] > calc_prob['low'] else 'low'\n",
    "        positives += prediction == 'high'\n",
    "        accuracy += prediction == label\n",
    "        tp += prediction == 'high' and label == 'high'\n",
    "        total_sentences += 1\n",
    "\n",
    "    acc_total = accuracy / total_sentences\n",
    "    precision = tp / positives if positives > 0 else 0\n",
    "    recall = tp / priors_count_test['high'] if priors_count_test['high'] > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f'El accuracy total es de {acc_total}')\n",
    "    print(f'La precision es de {precision}')\n",
    "    print(f'El recall es de {recall}')\n",
    "    print(f'El F1 score es de {f1}')\n",
    "    \n",
    "    if variation is None:\n",
    "        print('')\n",
    "    else:\n",
    "        results_df.loc[len(results_df)] = [variation, j+1, acc_total, precision, recall, f1]\n",
    "    return priors_probs, likelihood, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set N°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 32942.77it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [04:12<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8372235872235873\n",
      "La precision es de 0.8378712871287128\n",
      "El recall es de 0.997789240972734\n",
      "El F1 score es de 0.9108644466868484\n",
      "Train set N°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 33965.88it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [04:01<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8151105651105651\n",
      "La precision es de 0.8152709359605911\n",
      "El recall es de 0.999245283018868\n",
      "El F1 score es de 0.897931502204137\n",
      "Train set N°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 29478.87it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [03:42<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.828518746158574\n",
      "La precision es de 0.8300370828182941\n",
      "El recall es de 0.9970304380103935\n",
      "El F1 score es de 0.905902192242833\n",
      "Train set N°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 36884.18it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [03:53<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8303626306084819\n",
      "La precision es de 0.8306650246305419\n",
      "El recall es de 0.9992592592592593\n",
      "El F1 score es de 0.9071956960322799\n",
      "Train set N°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 38072.99it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [03:58<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8149969268592502\n",
      "La precision es de 0.8147004323656578\n",
      "El recall es de 0.9992424242424243\n",
      "El F1 score es de 0.8975842123171146\n"
     ]
    }
   ],
   "source": [
    "for j in range(0, 5):\n",
    "    train_test(no_pre, j, 'no pre-processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lemmatize = True, remove_stopwords = False):\n",
    "    doc = nlp(text.lower())\n",
    "    if lemmatize:\n",
    "        if remove_stopwords:\n",
    "            return [token.lemma_ for token in doc if not token.is_stop]\n",
    "        return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Data: 100%|██████████| 8137/8137 [03:59<00:00, 33.97it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = []\n",
    "for row in tqdm.tqdm(data, desc=\"Preprocessing Data\"):\n",
    "    preprocessed_data.append((preprocess_text(row[0], lemmatize = True, remove_stopwords = False), row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set N°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 29611.68it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [00:53<00:00, 30.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8409090909090909\n",
      "La precision es de 0.8635761589403973\n",
      "El recall es de 0.9609432571849669\n",
      "El F1 score es de 0.9096616672479944\n",
      "Train set N°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 38978.32it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [00:51<00:00, 31.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8267813267813268\n",
      "La precision es de 0.8428665351742275\n",
      "El recall es de 0.9675471698113207\n",
      "El F1 score es de 0.9009135628952916\n",
      "Train set N°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 36591.32it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [00:51<00:00, 31.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8401966810079902\n",
      "La precision es de 0.8536109303838647\n",
      "El recall es de 0.9740163325909429\n",
      "El F1 score es de 0.9098474341192787\n",
      "Train set N°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 39851.19it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [00:52<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8309772587584512\n",
      "La precision es de 0.8515369522563767\n",
      "El recall es de 0.9644444444444444\n",
      "El F1 score es de 0.9044807224730809\n",
      "Train set N°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 37170.84it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [00:53<00:00, 30.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8272894898586355\n",
      "La precision es de 0.8406557377049181\n",
      "El recall es de 0.9712121212121212\n",
      "El F1 score es de 0.9012302284710019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(0, 5):\n",
    "    train_test(preprocessed_data, j, 'lemmatization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Data:   0%|          | 0/8137 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Data: 100%|██████████| 8137/8137 [04:04<00:00, 33.31it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data_stopword = []\n",
    "for row in tqdm.tqdm(data, desc=\"Preprocessing Data\"):\n",
    "    preprocessed_data.append((preprocess_text(row[0], lemmatize=True, remove_stopwords=True), row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set N°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 45952.93it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [00:24<00:00, 66.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8421375921375921\n",
      "La precision es de 0.8632760898282695\n",
      "El recall es de 0.9631540162122328\n",
      "El F1 score es de 0.9104841518634623\n",
      "Train set N°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 59466.27it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [00:24<00:00, 67.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8200245700245701\n",
      "La precision es de 0.837696335078534\n",
      "El recall es de 0.9660377358490566\n",
      "El F1 score es de 0.8973010865755345\n",
      "Train set N°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 51990.75it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [00:23<00:00, 68.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8377381684081131\n",
      "La precision es de 0.8518518518518519\n",
      "El recall es de 0.9732739420935412\n",
      "El F1 score es de 0.9085239085239085\n",
      "Train set N°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 46839.13it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [00:27<00:00, 59.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8291333743085433\n",
      "La precision es de 0.8489583333333334\n",
      "El recall es de 0.965925925925926\n",
      "El F1 score es de 0.9036729036729036\n",
      "Train set N°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6510/6510 [00:00<00:00, 55461.05it/s]\n",
      "Testing: 100%|██████████| 1627/1627 [00:37<00:00, 43.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.826060233558697\n",
      "La precision es de 0.84\n",
      "El recall es de 0.9704545454545455\n",
      "El F1 score es de 0.9005272407732864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(0, 5):\n",
    "    train_test(preprocessed_data_stopword, j, 'lemmatization and stopword removal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results_variations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict all 'high'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to always predict 'high', what would out metrics be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8137/8137 [00:00<00:00, 903697.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'high': 0.8232763917905862, 'low': 0.1767236082094138}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "priors_count = defaultdict(int)\n",
    "for row in tqdm.tqdm(data):\n",
    "    label = row[1]\n",
    "    priors_count[label] += 1\n",
    "\n",
    "priors_probs = {cls: priors_count[cls] / sum(priors_count.values()) for cls in priors_count}\n",
    "print(priors_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an 82.3% probability that the review is 'high', so if we were to predict 'high' every time, we would be right 82% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8232763917905862\n"
     ]
    }
   ],
   "source": [
    "recall = priors_probs['high']*len(data)/len(data)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall would be the same as accuracy, because we'd get right about 82% of our predictions, which is 0.82*(number of predictions), and when we normalize by our (true positives + false positives), that is our number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8232763917905862\n"
     ]
    }
   ],
   "source": [
    "precision = priors_probs['high']*len(data)/(priors_probs['high']*len(data) + priors_probs['low']*len(data))\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, precision would be the same as accuracy and recall, because our false positives + our true positives are equal to our entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8232763917905862\n"
     ]
    }
   ],
   "source": [
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our F1 score would be the same as the accuracy, precision and recall. This gives us our benchmark so that we can say if our models are better than simply predicting 'high' all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models are slightly more accurate than the most basic \"predict 'high' all the time\". Precision is also a bit better, not too much difference. However, recall shows a great improvement. The tokenized, lemmatized and stopword removed models aren't that much better than the model without text pre-processing. However, the (slightly) best model is one with lemmatization and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set N°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6509/6509 [00:00<00:00, 31167.91it/s]\n",
      "Testing: 100%|██████████| 1628/1628 [00:26<00:00, 61.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy total es de 0.8421375921375921\n",
      "La precision es de 0.8632760898282695\n",
      "El recall es de 0.9631540162122328\n",
      "El F1 score es de 0.9104841518634623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "priors_probs, likelihood, vocabulary = train_test(preprocessed_data_stopword, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test =[]\n",
    "with open('yelp_reviews_test.csv', 'r') as csv_file:\n",
    "    file = csv.reader(csv_file)\n",
    "    for row in file:\n",
    "        data_test.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Data: 100%|██████████| 1863/1863 [00:59<00:00, 31.40it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_test = []\n",
    "for row in tqdm.tqdm(data_test, desc=\"Preprocessing Data\"):\n",
    "    preprocessed_test.append(preprocess_text(row[0], lemmatize=True, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:: 100%|██████████| 1863/1863 [00:30<00:00, 61.82it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for words in tqdm.tqdm(preprocessed_test, desc = 'Testing:'):\n",
    "    calc_prob = {cls: math.log(priors_probs[cls]) for cls in priors_probs}\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            for cls in priors_probs:\n",
    "                calc_prob[cls] += math.log(likelihood[cls][word])\n",
    "\n",
    "    prediction = 'high' if calc_prob['high'] > calc_prob['low'] else 'low'\n",
    "    predictions.append(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "i = 0\n",
    "for row in data_test:\n",
    "    final.append((row, predictions[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final, columns = ['text', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
